{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Required Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "import sys\n",
    "sys.path.append('./network/')\n",
    "from branched_network_class import branched_network\n",
    "import tensorflow as tf\n",
    "from scipy import signal\n",
    "import scipy.io.wavfile as wav\n",
    "import matplotlib as plt \n",
    "%pylab inline\n",
    "from pycochleagram import cochleagram as cgram \n",
    "from PIL import Image\n",
    "from pydub import AudioSegment\n",
    "import os\n",
    "import random\n",
    "from sphfile import SPHFile\n",
    "import string\n",
    "import shutil\n",
    "from heapq import nlargest\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions for setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    np.seterr(divide = 'ignore')\n",
    "    tf.reset_default_graph()\n",
    "    net_object = branched_network()\n",
    "    word_key = np.load('./demo_stim/logits_to_word_key.npy')\n",
    "    w1 = word_key[:242]\n",
    "    w2 = word_key[243:588]\n",
    "    word_key = np.concatenate((w1, w2))\n",
    "    new_key = []\n",
    "    trans = str.maketrans('', '', string.punctuation)\n",
    "    for word in word_key:\n",
    "        new_key.append(word.decode('UTF-8').lower().translate(trans).strip())\n",
    "    return net_object, new_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_word_bank():\n",
    "    bank = set()\n",
    "    trans = str.maketrans('', '', string.punctuation)\n",
    "    with open('wordbank.txt','r') as f:\n",
    "        for line in f:\n",
    "            for word in line.split():\n",
    "                bank.add(word.lower().translate(trans).strip())\n",
    "    return bank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions for file processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_wav(sr, wav_f, offset): #offset assumes sr of 16000\n",
    "    min_len = 2 * sr\n",
    "    if len(wav_f) < min_len + offset:\n",
    "        buf = min_len + offset - len(wav_f)\n",
    "        silence = np.zeros((buf,), dtype = int)\n",
    "        wav_f = np.concatenate((wav_f, silence))\n",
    "    cut = wav_f[offset : offset + 2 * sr]\n",
    "    return cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample(example, new_size):\n",
    "    im = Image.fromarray(example)\n",
    "    resized_image = im.resize(new_size, resample=Image.ANTIALIAS)\n",
    "    return np.array(resized_image)\n",
    "\n",
    "def plot_cochleagram(cochleagram, title): \n",
    "    plt.figure(figsize=(6,3))\n",
    "    plt.matshow(cochleagram.reshape(256,256), origin='lower',cmap=plt.cm.Blues, fignum=False, aspect='auto')\n",
    "    plt.yticks([]); plt.xticks([]); plt.title(title); \n",
    "    \n",
    "def play_wav(wav_f, sr, title):   \n",
    "    print (title+':')\n",
    "    ipd.display(ipd.Audio(wav_f, rate=sr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cochleagram(wav_f, sr, title):\n",
    "    # define parameters\n",
    "    n, sampling_rate = 50, 16000\n",
    "    low_lim, hi_lim = 20, 8000\n",
    "    sample_factor, pad_factor, downsample = 4, 2, 200\n",
    "    nonlinearity, fft_mode, ret_mode = 'power', 'auto', 'envs'\n",
    "    strict = True\n",
    "\n",
    "    # create cochleagram\n",
    "    #print(type(wav_f))\n",
    "    #print(wav_f.shape)\n",
    "    c_gram = cgram.cochleagram(wav_f, sr, n, low_lim, hi_lim, \n",
    "                               sample_factor, pad_factor, downsample,\n",
    "                               nonlinearity, fft_mode, ret_mode, strict)\n",
    "    #frequencies, times, c_gram = signal.spectrogram(wav_f, sr)\n",
    "    \n",
    "    # rescale to [0,255]\n",
    "    c_gram_rescaled =  255*(1-((np.max(c_gram)-c_gram)/np.ptp(c_gram)))\n",
    "    #print(type(c_gram_rescaled))\n",
    "    \n",
    "    # reshape to (256,256)\n",
    "    c_gram_reshape_1 = np.reshape(c_gram_rescaled, (211,400))\n",
    "    c_gram_reshape_2 = resample(c_gram_reshape_1,(256,256))\n",
    "    \n",
    "    #plot_cochleagram(c_gram_reshape_2, title)\n",
    "\n",
    "    # prepare to run through network -- i.e., flatten it\n",
    "    c_gram_flatten = np.reshape(c_gram_reshape_2, (1, 256*256)) \n",
    "    \n",
    "    return c_gram_flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_bank(word, bank):\n",
    "    trans = str.maketrans('', '', string.punctuation)\n",
    "    return word.lower().translate(trans).strip() in bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracts full string, speaker detail string, title, and offset\n",
    "def extract_details(f):\n",
    "    info = f[:f.index(\".\")]\n",
    "    sp_des = info[:5]\n",
    "    f = info[5:]\n",
    "    title = f[:f.index(\"-\")]\n",
    "    offset = f[f.index(\"-\") + 1 :]\n",
    "    return sp_des, title, int(offset)\n",
    "\n",
    "def extract_text(f):\n",
    "    info = f[:f.index(\".\")]\n",
    "    sp_des = info[:5]\n",
    "    f = info[5:]\n",
    "    return sp_des, f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_txt(path, bank): #reads the transcript of a txt file\n",
    "    with open(path, encoding = 'us-ascii') as t:\n",
    "        tscript = t.readlines()[0]\n",
    "    tscript = tscript.split(' ')[2:]\n",
    "    base = ''\n",
    "    trans = str.maketrans('', '', string.punctuation)\n",
    "    word_set = dict()\n",
    "    for i in range(len(tscript)):\n",
    "        x = tscript[i]\n",
    "        base += x + ' '\n",
    "        basic = x.lower().translate(trans).strip()\n",
    "        if check_bank(basic, bank):\n",
    "            if basic in word_set:\n",
    "                word_set[basic].append(i + 1)\n",
    "            else:\n",
    "                word_set[basic] = [i + 1]           \n",
    "    base = base[:len(base) - 1]\n",
    "    return base, word_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_txt(path, bank, mini, maxi): #reads and checks whther banked words are within [mini, maxi]\n",
    "    with open(path, encoding = 'us-ascii') as t:\n",
    "        tscript = t.readlines()[0]\n",
    "    tscript = tscript.split(' ')[2:]\n",
    "    base = ''\n",
    "    trans = str.maketrans('', '', string.punctuation)\n",
    "    word_set = dict()\n",
    "    if len(tscript) < mini:\n",
    "        return False\n",
    "    for i in range(min(mini - 1, len(tscript) - 1), min(maxi, len(tscript))):\n",
    "        x = tscript[i]\n",
    "        #print(x)\n",
    "        base += x + ' '\n",
    "        basic = x.lower().translate(trans).strip()\n",
    "        if check_bank(basic, bank):\n",
    "            if basic in word_set:\n",
    "                word_set[basic].append(i + 1)\n",
    "            else:\n",
    "                word_set[basic] = [i + 1]        \n",
    "    base = base[:len(base) - 1]\n",
    "    if len(word_set) == 0:\n",
    "        return False\n",
    "    #print(base)\n",
    "    return word_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions for calculating and displaying results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_file(sr, wav_f, title, model):\n",
    "    c_gram = generate_cochleagram(wav_f, sr, title)\n",
    "    logits = model.session.run(model.word_logits, feed_dict={model.x: c_gram})\n",
    "    l1 = logits[0][:242]\n",
    "    l2 = logits[0][243:588]\n",
    "    return np.concatenate((l1, l2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cgram(cgram, model):\n",
    "    logits = model.session.run(model.word_logits, feed_dict={model.x: cgram})\n",
    "    l1 = logits[0][:242]\n",
    "    l2 = logits[0][243:588]\n",
    "    return np.concatenate((l1, l2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distrib(logits, n): #gets top n outcomes\n",
    "    inds = np.argpartition(logits, -1 * n)[-1 * n:]\n",
    "    return inds[np.argsort(logits[inds])][::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions for moving or converting files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dr_conv(dr):\n",
    "    #Used to convert SPH files to wavs in each subdirectory in training dataset\n",
    "    path = \"./TIMIT/TEST/\" + f\"DR{dr}\" #shouldnt ever need to do again but can change back to train\n",
    "    for subdir in os.listdir(path):\n",
    "        new_path = os.path.join(path, subdir)\n",
    "        #print(new_path)\n",
    "        if os.path.isdir(new_path):\n",
    "            #print(subdir)\n",
    "            wavpath = new_path + '/wavs'\n",
    "            os.mkdir(wavpath)\n",
    "            for filename in os.listdir(new_path):\n",
    "                if filename.lower().endswith('.wav'):\n",
    "                    audiopath = os.path.join(new_path, filename)\n",
    "                    #print(audiopath)\n",
    "                    sph = SPHFile(audiopath)\n",
    "                    sph.write_wav(os.path.join(new_path + '/wavs', filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#semi-thoroughly tests all wav's in a single folder of timit data\n",
    "def timit_folder_test(dr, folder):\n",
    "    or_path = \"./TIMIT/TRAIN/\" + f\"DR{dr}/\" + folder\n",
    "    full_path = or_path + \"/wavs\"\n",
    "    print(f'Testing all files in {full_path}\\n')\n",
    "    np.seterr(divide = 'ignore')\n",
    "    tf.reset_default_graph()\n",
    "    net_object = branched_network()\n",
    "    word_key = np.load('./demo_stim/logits_to_word_key.npy')\n",
    "    w1 = word_key[:242]\n",
    "    w2 = word_key[243:588]\n",
    "    word_key = np.concatenate((w1, w2))\n",
    "    for filename in os.listdir(full_path):\n",
    "        print(filename)\n",
    "        title = filename[:filename.index(\".\")]\n",
    "        print(f'The file identifier is {title}.')\n",
    "        audiopath = os.path.join(full_path, filename)\n",
    "        textpath = os.path.join(or_path, title + '.txt')\n",
    "        #print(textpath)\n",
    "        with open(textpath, encoding = 'us-ascii') as t:\n",
    "            tscript = t.readlines()[0]\n",
    "        tscript = tscript.split(' ')[2:]\n",
    "        base = ''\n",
    "        for x in tscript:\n",
    "            base += x + ' '\n",
    "        base = base[:len(base) - 1]\n",
    "        print(f'The sentence transcription is: {base}')\n",
    "        min_len = 32000\n",
    "        sr, wav_f = wav.read(audiopath)\n",
    "        for i in range(1): #offset from 0 to .5 seconds to explore window\n",
    "            #can change range to 0 for one iteration\n",
    "            offset = i * (sr // 10)\n",
    "            print(f'Testing offset of {i / 10} seconds.\\n')\n",
    "            if len(wav_f) < min_len + offset:\n",
    "                buf = min_len + offset - len(wav_f)\n",
    "                silence = np.zeros((buf,), dtype = int)\n",
    "                wav_f = np.concatenate((wav_f, silence))\n",
    "            cut = wav_f[offset : offset + 2 * sr]\n",
    "            #print(cut.shape)\n",
    "            c_gram = generate_cochleagram(cut, sr, f'{audiopath}')\n",
    "            logits = net_object.session.run(net_object.word_logits, feed_dict={net_object.x: c_gram})\n",
    "            l1 = logits[0][:242]\n",
    "            l2 = logits[0][243:588]\n",
    "            logits = np.concatenate((l1, l2))\n",
    "            #printing top 5 distribution\n",
    "            inds = np.argpartition(logits, -5)[-5:]\n",
    "            inds = inds[np.argsort(logits[inds])][::-1]\n",
    "            for i in range(5):\n",
    "                pred = word_key[inds[i]].decode('UTF-8')\n",
    "                print(f\"Result #{i + 1}: {pred}\")\n",
    "            print('\\n')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timit_dr_test(dr):\n",
    "    dr_path = \"./TIMIT/TRAIN/\" + f\"DR{dr}\"\n",
    "    print(f'Testing all files in DR{dr}\\n')\n",
    "    np.seterr(divide = 'ignore')\n",
    "    tf.reset_default_graph()\n",
    "    net_object = branched_network()\n",
    "    word_key = np.load('./demo_stim/logits_to_word_key.npy')\n",
    "    w1 = word_key[:242]\n",
    "    w2 = word_key[243:588]\n",
    "    word_key = np.concatenate((w1, w2))\n",
    "    for sd in os.listdir(dr_path):\n",
    "        set_path = os.path.join(dr_path, sd)\n",
    "        if os.path.isdir(set_path):\n",
    "            print(f'Testing the files in {set_path}\\n')\n",
    "            full_path = os.path.join(set_path, 'wavs')\n",
    "            for filename in os.listdir(full_path):\n",
    "                #print(filename)\n",
    "                title = filename[:filename.index(\".\")]\n",
    "                print(f'The file identifier is {title}.')\n",
    "                audiopath = os.path.join(full_path, filename)\n",
    "                textpath = os.path.join(set_path, title + '.txt')\n",
    "                #print(textpath)\n",
    "                with open(textpath, encoding = 'us-ascii') as t:\n",
    "                    tscript = t.readlines()[0]\n",
    "                tscript = tscript.split(' ')[2:]\n",
    "                base = ''\n",
    "                for x in tscript:\n",
    "                    base += x + ' '\n",
    "                base = base[:len(base) - 1]\n",
    "                print(f'The sentence transcription is: {base}')\n",
    "                min_len = 32000\n",
    "                sr, wav_f = wav.read(audiopath)\n",
    "                for i in range(1): #offset from 0 to .5 seconds to explore window\n",
    "                    #can change range to 0 for one iteration\n",
    "                    offset = i * (sr // 10)\n",
    "                    print(f'Testing offset of {i / 10} seconds.\\n')\n",
    "                    if len(wav_f) < min_len + offset:\n",
    "                        buf = min_len + offset - len(wav_f)\n",
    "                        silence = np.zeros((buf,), dtype = int)\n",
    "                        wav_f = np.concatenate((wav_f, silence))\n",
    "                    cut = wav_f[offset : offset + 2 * sr]\n",
    "                    #print(cut.shape)\n",
    "                    c_gram = generate_cochleagram(cut, sr, f'{audiopath}')\n",
    "                    logits = net_object.session.run(net_object.word_logits, feed_dict={net_object.x: c_gram})\n",
    "                    l1 = logits[0][:242]\n",
    "                    l2 = logits[0][243:588]\n",
    "                    logits = np.concatenate((l1, l2))\n",
    "                    #printing top 5 distribution\n",
    "                    inds = np.argpartition(logits, -5)[-5:]\n",
    "                    inds = inds[np.argsort(logits[inds])][::-1]\n",
    "                    for i in range(5):\n",
    "                        pred = word_key[inds[i]].decode('UTF-8')\n",
    "                        print(f\"Result #{i + 1}: {pred}\")\n",
    "                    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dialect_test(n):\n",
    "    #Build the unchanging parts of the model\n",
    "    net_object, word_key = load_model()\n",
    "    word_bank = build_word_bank()\n",
    "    \n",
    "    #Loop through the 8 dialects\n",
    "    base_dir = \"./TIMIT/TRAIN\"\n",
    "    for i in range(1,9):\n",
    "        dr_path = os.path.join(base_dir, f'DR{i}')\n",
    "        print(f'Sampling from dialect {i}.\\n')\n",
    "\n",
    "        #Reset success and fail values\n",
    "        s_count = 0\n",
    "        f_count = 0\n",
    "        \n",
    "        #sample from n different folders to ensure uniquity\n",
    "        sds = random.sample(os.listdir(dr_path), n)\n",
    "        for sd in sds:\n",
    "            set_path = os.path.join(dr_path, sd)\n",
    "            if os.path.isdir(set_path):\n",
    "                full_path = os.path.join(set_path, 'wavs')\n",
    "                files = os.listdir(full_path)\n",
    "\n",
    "                #bounce the shared sentences\n",
    "                files.remove('SA1.WAV')\n",
    "                files.remove('SA2.WAV')\n",
    "\n",
    "                #select random audio file\n",
    "                fd = random.sample(files, 1)[0] #change number of looks here\n",
    "                title = fd[:fd.index(\".\")]\n",
    "                print(f'Reading from set {sd}. The file identifier is {title}.')\n",
    "                audiopath = os.path.join(full_path, fd)\n",
    "                textpath = os.path.join(set_path, title + '.txt')\n",
    "\n",
    "                #display transcription\n",
    "                with open(textpath, encoding = 'us-ascii') as t:\n",
    "                    tscript = t.readlines()[0]\n",
    "                tscript = tscript.split(' ')[2:]\n",
    "                base = ''\n",
    "                trans = str.maketrans('', '', string.punctuation)\n",
    "                word_set = set()\n",
    "                flag = False\n",
    "                for x in tscript:\n",
    "                    base += x + ' '\n",
    "                    basic = x.lower().translate(trans).strip()\n",
    "                    word_set.add(basic)\n",
    "                    if not flag:\n",
    "                        if check_bank(basic, word_bank):\n",
    "                            flag = True\n",
    "                if not flag:\n",
    "                    print('No words in training set.\\n')\n",
    "                    continue\n",
    "                base = base[:len(base) - 1]\n",
    "                print(f'The sentence transcription is: {base}')\n",
    "\n",
    "                #pad out file if necessary\n",
    "                sr, wav_f = wav.read(audiopath)\n",
    "                wav_f = process_wav(sr, wav_f, 0) #no offset\n",
    "\n",
    "                #generate cochleagram and run through model\n",
    "                logits = run_file(sr, wav_f, f'{audiopath}', net_object)\n",
    "\n",
    "                #get top 5 results\n",
    "                inds = distrib(logits, 5)\n",
    "\n",
    "                #measure success or failure\n",
    "                flag = False\n",
    "                for j in range(5):\n",
    "                    pred = word_key[inds[j]].decode('UTF-8')\n",
    "                    pred = pred.lower().translate(trans).strip()\n",
    "                    if pred in word_set:\n",
    "                        flag = True\n",
    "                        print(f\"Success, '{pred}' was correctly identified.\\n\")\n",
    "                        s_count += 1\n",
    "                        break\n",
    "                if flag == False:\n",
    "                    print('Failure.\\n')\n",
    "                    f_count += 1\n",
    "        print(f\"\\nResult for dialect {i}: {s_count} successes out of {s_count + f_count} attempts.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dialect_random_test(n):\n",
    "    #Build the unchanging parts of the model\n",
    "    net_object, word_key = load_model()\n",
    "    word_bank = build_word_bank()\n",
    "    \n",
    "    #Loop through the 8 dialects\n",
    "    base_dir = \"./TIMIT/TRAIN\"\n",
    "    for i in range(1,9):\n",
    "        dr_path = os.path.join(base_dir, f'DR{i}')\n",
    "        audio_set = []\n",
    "        print(f'Sampling from dialect {i}.\\n')\n",
    "\n",
    "        #Reset success and fail values\n",
    "        s_count = 0\n",
    "        t_count = 0\n",
    "        \n",
    "        #build full set of audio options\n",
    "        for sd in os.listdir(dr_path):\n",
    "            set_path = os.path.join(dr_path, sd)\n",
    "            if os.path.isdir(set_path):\n",
    "                full_path = os.path.join(set_path, 'wavs')\n",
    "                files = os.listdir(full_path)\n",
    "                files.remove('SA1.WAV')\n",
    "                files.remove('SA2.WAV')\n",
    "                for f in files:\n",
    "                    title = f[:f.index(\".\")]\n",
    "                    audiopath = os.path.join(full_path, f)\n",
    "                    textpath = os.path.join(set_path, title + '.txt')\n",
    "                    audio_set.append((audiopath, textpath))\n",
    "        \n",
    "\n",
    "        #select n valid files\n",
    "        while t_count < n and len(audio_set) != 0:\n",
    "            fd = random.choice(audio_set)\n",
    "            audio_set.remove(fd)\n",
    "            audiopath = fd[0]\n",
    "            textpath = fd[1]\n",
    "\n",
    "            #check transcript and display\n",
    "            with open(textpath, encoding = 'us-ascii') as t:\n",
    "                tscript = t.readlines()[0]\n",
    "            tscript = tscript.split(' ')[2:]\n",
    "            base = ''\n",
    "            trans = str.maketrans('', '', string.punctuation)\n",
    "            word_set = set()\n",
    "            flag = 0\n",
    "            for x in tscript:\n",
    "                base += x + ' '\n",
    "                basic = x.lower().translate(trans).strip()\n",
    "                word_set.add(basic)\n",
    "                if flag != 2:\n",
    "                    if check_bank(basic, word_bank):\n",
    "                        flag += 1\n",
    "            if flag != 2:\n",
    "                print('Not enough words in training set.\\n')\n",
    "                continue\n",
    "            base = base[:len(base) - 1]\n",
    "            print(f'The sentence transcription is: {base}')\n",
    "\n",
    "            #pad out file if necessary\n",
    "            sr, wav_f = wav.read(audiopath)\n",
    "            wav_f = process_wav(sr, wav_f, 0) #no offset\n",
    "\n",
    "            #generate cochleagram and run through model\n",
    "            logits = run_file(sr, wav_f, f'{audiopath}', net_object)\n",
    "\n",
    "            #get top 5 results\n",
    "            inds = distrib(logits, 5)\n",
    "\n",
    "                #measure success or failure\n",
    "            flag = False\n",
    "            for j in range(5):\n",
    "                pred = word_key[inds[j]].decode('UTF-8')\n",
    "                pred = pred.lower().translate(trans).strip()\n",
    "                if pred in word_set:\n",
    "                    flag = True\n",
    "                    print(f\"Success, '{pred}' was correctly identified.\\n\")\n",
    "                    s_count += 1\n",
    "                    t_count += 1\n",
    "                    break\n",
    "            if flag == False:\n",
    "                print('Failure.\\n')\n",
    "                t_count += 1\n",
    "\n",
    "        print(f\"\\nResult for dialect {i}: {s_count} successes out of {t_count} attempts.\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_core(dr, c1, c2, c3): #dr is the dialect, c1, c2 are the male core speakers, c3 is the female core speaker\n",
    "    #Build the unchanging parts of the model\n",
    "    net_object, word_key = load_model()\n",
    "    word_bank = build_word_bank()\n",
    "    dr_path = os.path.join(\"./TIMIT/TEST\", f\"DR{dr}\")\n",
    "    speakers = [f\"M{c1}\", f\"M{c2}\", f\"F{c3}\"]\n",
    "\n",
    "    #Loop through the 3 core speakers\n",
    "    for sd in os.listdir(dr_path):\n",
    "        if sd in speakers:\n",
    "            print(f\"Testing core speaker {sd}.\")\n",
    "            base_path = os.path.join(dr_path, sd)\n",
    "            full_path = os.path.join(base_path, 'wavs')\n",
    "            \n",
    "            #Reset success and fail values\n",
    "            s_count = 0\n",
    "            t_count = 0\n",
    "\n",
    "            #Loop through wavs\n",
    "            for f in os.listdir(full_path):\n",
    "                title = f[:f.index(\".\")]\n",
    "                audiopath = os.path.join(full_path, f)\n",
    "                textpath = os.path.join(base_path, title + '.txt')\n",
    "                print(f'The file identifier is {title}.')\n",
    "\n",
    "                with open(textpath, encoding = 'us-ascii') as t:\n",
    "                    tscript = t.readlines()[0]\n",
    "                tscript = tscript.split(' ')[2:]\n",
    "                base = ''\n",
    "                trans = str.maketrans('', '', string.punctuation)\n",
    "                word_set = set()\n",
    "                for x in tscript:\n",
    "                    base += x + ' '\n",
    "                    basic = x.lower().translate(trans).strip()\n",
    "                    if check_bank(basic, word_bank):\n",
    "                        word_set.add(basic)\n",
    "                \n",
    "                base = base[:len(base) - 1]\n",
    "                print(f'The sentence transcription is: {base}')\n",
    "                if len(word_set) == 0:\n",
    "                    print('No words in training set.\\n')\n",
    "                    continue\n",
    "                print(f'Words in the training set are {word_set}.')\n",
    "\n",
    "                #pad out file if necessary\n",
    "                sr, wav_f = wav.read(audiopath)\n",
    "                wav_f = process_wav(sr, wav_f, 0) #no offset\n",
    "\n",
    "                #generate cochleagram and run through model\n",
    "                logits = run_file(sr, wav_f, f'{title}', net_object)\n",
    "\n",
    "                #get top 5 results\n",
    "                inds = distrib(logits, 5)\n",
    "\n",
    "                #interpret results\n",
    "                flag = False\n",
    "                for j in range(5):\n",
    "                    pred = word_key[inds[j]].decode('UTF-8')\n",
    "                    print(f\"Result #{j + 1}: {pred}\")\n",
    "                    pred = pred.lower().translate(trans).strip()\n",
    "                    if pred in word_set:\n",
    "                        flag = True\n",
    "                        print(f\"Success, '{pred}' was correctly identified.\")\n",
    "                t_count += 1\n",
    "                if flag == False:\n",
    "                    print('Failure.\\n')\n",
    "                else:\n",
    "                    print('Success.\\n')\n",
    "                    s_count += 1\n",
    "            print(f\"\\nResult for speaker {sd}: {s_count} successes out of {t_count} attempts.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finds the position distribution of all correctly identified words in files with a certain offset in dr\n",
    "def get_pos(dr, p_range, offset):\n",
    "    #Build the unchanging parts of the model\n",
    "    net_object, word_key = load_model()\n",
    "    word_bank = build_word_bank()\n",
    "\n",
    "    #set necessary paths\n",
    "    bp = os.path.join(\"graph_data\", f\"dr{dr}\")\n",
    "    t_dir = os.path.join(bp, \"txts\")\n",
    "    c_dir = os.path.join(bp, \"cgrams\")\n",
    "    c_dir = os.path.join(c_dir, f\"{offset}\")\n",
    "\n",
    "    #initialize data structures\n",
    "    pos = dict()\n",
    "    certs = dict()\n",
    "    s_count = 0\n",
    "    total = 0\n",
    "\n",
    "    #loop through files in directory\n",
    "    for fd in os.listdir(c_dir):\n",
    "        #run through model\n",
    "        sp_des, title, o = extract_details(fd)\n",
    "        info = sp_des + title\n",
    "        textpath = os.path.join(t_dir, f\"{info}.txt\")\n",
    "        base, word_set = read_txt(textpath, word_bank)\n",
    "        #print(f\"{word_set} identifiable in '{base}'\")\n",
    "        #print(c_dir, fd)\n",
    "        cgram = np.load(os.path.join(c_dir, fd))\n",
    "        logits = run_cgram(cgram, net_object)\n",
    "        inds = distrib(logits, p_range)\n",
    "\n",
    "        #interpret results\n",
    "        flag = False\n",
    "        trans = str.maketrans('', '', string.punctuation)\n",
    "        for j in range(p_range):\n",
    "            pred = word_key[inds[j]]\n",
    "            pred = pred.lower().translate(trans).strip()\n",
    "            if pred in word_set:\n",
    "                flag = True\n",
    "                index = word_set[pred][0]\n",
    "                #print(f\"Success, '{pred}' was correctly identified in position {index}, with certainty of {j+1}.\\n\")\n",
    "                if (j + 1) in certs:\n",
    "                    certs[j+1] += 1\n",
    "                else:\n",
    "                    certs[j+1] = 1\n",
    "                if index in pos:\n",
    "                        pos[index] += 1\n",
    "                else:\n",
    "                    pos[index] = 1\n",
    "                break\n",
    "        total += 1\n",
    "        if flag == False:\n",
    "            pass\n",
    "            #print('Failure.\\n')\n",
    "        else:\n",
    "            s_count += 1\n",
    "    print(f\"Ratio: {s_count} / {total}\")\n",
    "    print(f'Positionings: {pos}')\n",
    "    print(f'Certainties: {certs}\\n')\n",
    "    #return pos, certs, s_count, total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reform_data(s, dr, offset):\n",
    "    print(f\"Reforming dr{dr} with offset {offset}.\")\n",
    "    np.seterr(divide = 'ignore')\n",
    "    dest = os.path.join(\"graph_data\", f\"dr{dr}\")\n",
    "    #os.mkdir(dest)\n",
    "    a_dir = os.path.join(dest, \"wavs\")\n",
    "    t_dir = os.path.join(dest, \"txts\")\n",
    "    c_dir = os.path.join(dest, \"cgrams\")\n",
    "    if not os.path.isdir(a_dir):\n",
    "        os.mkdir(a_dir)\n",
    "    if not os.path.isdir(t_dir):\n",
    "        os.mkdir(t_dir)\n",
    "    if not os.path.isdir(c_dir):\n",
    "        os.mkdir(c_dir)\n",
    "    o_dir = os.path.join(c_dir, f\"{offset}\")\n",
    "    if not os.path.isdir(o_dir):\n",
    "        os.mkdir(o_dir)\n",
    "    dr_path = os.path.join(f\"./TIMIT/{s}\", f\"DR{dr}\")\n",
    "    word_bank = build_word_bank()\n",
    "\n",
    "    sdc = 0\n",
    "\n",
    "    for sd in os.listdir(dr_path):\n",
    "        base_path = os.path.join(dr_path, sd)\n",
    "        if os.path.isdir(base_path):\n",
    "            sdc += 1\n",
    "            print(f\"Entering subdirectory number {sdc}; {sd}.\")\n",
    "            full_path = os.path.join(base_path, 'wavs')\n",
    "            for f in os.listdir(full_path):\n",
    "                frag = f[:f.index(\".\")]\n",
    "                title = sd + frag\n",
    "\n",
    "                audiopath = os.path.join(full_path, f)\n",
    "                textpath = os.path.join(base_path, frag + '.txt')\n",
    "\n",
    "                #check validity\n",
    "                with open(textpath, encoding = 'us-ascii') as t:\n",
    "                    tscript = t.readlines()[0]\n",
    "                tscript = tscript.split(' ')[2:]\n",
    "                trans = str.maketrans('', '', string.punctuation)\n",
    "                word_set = set()\n",
    "\n",
    "                for i in range(len(tscript)):\n",
    "                    x = tscript[i]\n",
    "                    basic = x.lower().translate(trans).strip()\n",
    "                    if check_bank(basic, word_bank):\n",
    "                        word_set.add(basic)\n",
    "                        \n",
    "                if len(word_set) == 0:\n",
    "                            #print(f'No banked words for {title}.')\n",
    "                            continue\n",
    "                \n",
    "\n",
    "                #print(f\"{word_set} banked for {title}\")\n",
    "\n",
    "                sr, wav_f = wav.read(audiopath)\n",
    "                wav_f = process_wav(sr, wav_f, offset)\n",
    "                c_gram = generate_cochleagram(wav_f, sr, title)\n",
    "                fname = os.path.join(a_dir, f\"{title}.wav\")\n",
    "                if not os.path.isfile(fname):\n",
    "                    shutil.copy(audiopath, fname)\n",
    "                tname = os.path.join(t_dir, f\"{title}.txt\")\n",
    "                if not os.path.isfile(tname):\n",
    "                    shutil.copy(textpath, tname)\n",
    "                cname = os.path.join(o_dir, f\"{title}-{offset}.npy\")\n",
    "                np.save(cname, c_gram)\n",
    "                #print(f\"Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def result_dist(word_set, word_key, inds):\n",
    "    #returns true if one of the targets has been found\n",
    "    #print(word_set)\n",
    "    res = []\n",
    "    trans = str.maketrans('', '', string.punctuation)\n",
    "    for i in range(len(inds)):\n",
    "        res.append(inds[i])\n",
    "        pred = word_key[inds[i]]\n",
    "        pred = pred.lower().translate(trans).strip()\n",
    "        #print(pred)\n",
    "        if pred in word_set:\n",
    "            return True, res\n",
    "    return False, res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dist(results, target, dist):\n",
    "    if target in results:\n",
    "        dictA = results[target]\n",
    "    else:\n",
    "        dictA = dict()\n",
    "    l = len(dist)\n",
    "    scale = (l**2 + l) // 2\n",
    "    for i in range(l):\n",
    "        place = l - i\n",
    "        val = dist[i]\n",
    "        dictA[val] = place / scale\n",
    "    results[target] = dictA\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#builds a graph for just one dr and just one offset\n",
    "def build_standard_graph(dr, p_range, offset):\n",
    "    #Build the unchanging parts of the model\n",
    "    net_object, word_key = load_model()\n",
    "    word_bank = build_word_bank()\n",
    "\n",
    "    #set necessary paths\n",
    "    bp = os.path.join(\"graph_data\", f\"dr{dr}\")\n",
    "    t_dir = os.path.join(bp, \"txts\")\n",
    "    c_dir = os.path.join(bp, \"cgrams\")\n",
    "    c_dir = os.path.join(c_dir, f\"{offset}\")\n",
    "\n",
    "    #initialize data structures\n",
    "    results = dict()\n",
    "    cgram_set = []\n",
    "    #print(word_bank)\n",
    "\n",
    "    #loop through files in directory\n",
    "    for f in os.listdir(c_dir):\n",
    "        c_path = os.path.join(c_dir, f)\n",
    "        sp_des, title, o = extract_details(f)\n",
    "        info = sp_des + title\n",
    "        t_path = os.path.join(t_dir, f\"{info}.txt\")\n",
    "        s = check_txt(t_path, word_bank, 3, 5)\n",
    "        if s:\n",
    "            cgram_set.append((c_path, s))\n",
    "            #print(s)\n",
    "    #print(len(cgram_set))\n",
    "\n",
    "    for c in cgram_set:\n",
    "        cgram = np.load(c[0])\n",
    "        word_set = c[1]\n",
    "        logits = run_cgram(cgram, net_object)\n",
    "        inds = distrib(logits, p_range)\n",
    "\n",
    "        #scenario 1\n",
    "        res, dist = result_dist(word_set, word_key, inds)\n",
    "        #print(res)\n",
    "        if len(word_set) == 1:\n",
    "            for key in word_set:\n",
    "                target = word_key.index(key)\n",
    "            results = merge_dist(results, target, dist)\n",
    "\n",
    "        else:\n",
    "            res, dist = result_dist(word_set, word_key, inds)\n",
    "            if res:\n",
    "                target = dist[len(dist) - 1]\n",
    "                results = merge_dist(results, target, dist)\n",
    "            else:\n",
    "                for key in word_set:\n",
    "                    target = word_key.index(key)\n",
    "                    results = merge_dist(results, target, dist)\n",
    "\n",
    "    return(results)\n",
    "        #interpret result\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary(results):\n",
    "    word_key = np.load('./demo_stim/logits_to_word_key.npy')\n",
    "    #convert to binary\n",
    "    for key in results:\n",
    "        dictA = results[key]\n",
    "        tVal = max(dictA, key= dictA.get)\n",
    "        results[key] = tVal\n",
    "        #print(key == tVal)\n",
    "\n",
    "    #convert to numpy array\n",
    "    #x_words = set()\n",
    "    #y_words = set()\n",
    "    words = set()\n",
    "    for key in results:\n",
    "        words.add(key)\n",
    "        words.add(results[key])\n",
    "\n",
    "    labels = sorted(list(words))\n",
    "\n",
    "    m = len(labels)\n",
    "\n",
    "    #rows results, columns expected\n",
    "    data = np.zeros((m, m), dtype = int)\n",
    "\n",
    "    for key in results:\n",
    "        xpos = labels.index(key)\n",
    "        ypos = labels.index(results[key])\n",
    "        data[ypos][xpos] = 1\n",
    "\n",
    "    for i in range(len(labels)):\n",
    "        labels[i] = word_key[labels[i]].decode('UTF-8')\n",
    "    return data, labels\n",
    "    #return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def results_to_nway(results, n):\n",
    "    for key in results:\n",
    "        dictA = results[key]\n",
    "        tVal = nlargest(n, dictA, key = dictA.get)\n",
    "        for i in range(len(tVal)):\n",
    "            tVal[i] = (tVal[i], dictA[tVal[i]])\n",
    "        results[key] = tVal\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_way(results, n):\n",
    "    word_key = np.load('./demo_stim/logits_to_word_key.npy')\n",
    "    #convert to binary\n",
    "    results = results_to_nway(results, n)\n",
    "\n",
    "    #convert to numpy array\n",
    "    #x_words = set()\n",
    "    #y_words = set()\n",
    "    words = set()\n",
    "    for keys in results:\n",
    "        words.add(keys)\n",
    "        for key in results[keys]:\n",
    "            words.add(key[0])\n",
    "\n",
    "    labels = sorted(list(words))\n",
    "\n",
    "    m = len(labels)\n",
    "    #print(m)\n",
    "\n",
    "    #rows results, columns expected\n",
    "    data = np.zeros((m, m))\n",
    "\n",
    "    for key in results:\n",
    "        sub_arr = results[key]\n",
    "        xpos = labels.index(key)\n",
    "        s = 0\n",
    "        for i in range(len(sub_arr)):\n",
    "            s += sub_arr[i][1]\n",
    "\n",
    "        for i in range(len(sub_arr)):\n",
    "            ypos = labels.index(sub_arr[i][0])\n",
    "            data[ypos][xpos] = sub_arr[i][1] / s\n",
    "            \n",
    "            \n",
    "            #ypos = labels.index(results[key][i])\n",
    "            \n",
    "            #data[ypos][xpos] = 1 / (i + 1)\n",
    "\n",
    "    for i in range(len(labels)):\n",
    "        labels[i] = word_key[labels[i]].decode('UTF-8')\n",
    "\n",
    "    #print(labels)\n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_heatmap(data, x_labels, dr):\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(data)\n",
    "\n",
    "    ax.set_xticks(np.arange(data.shape[1]))\n",
    "    ax.set_xticklabels(x_labels, rotation=-90)\n",
    "    ax.set_yticks(np.arange(data.shape[0]))\n",
    "    ax.set_yticklabels(x_labels)\n",
    "    ax.set_title(f\"Binary Results for DR {dr}\")\n",
    "    fig.set_size_inches(16, 16)\n",
    "    fig.savefig(f\"dr{dr}-binary.png\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_nway(data, x_labels, dr, n):\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(data)\n",
    "\n",
    "    ax.set_xticks(np.arange(data.shape[1]))\n",
    "    ax.set_xticklabels(x_labels, rotation=-90)\n",
    "    ax.set_yticks(np.arange(data.shape[0]))\n",
    "    ax.set_yticklabels(x_labels)\n",
    "    ax.set_title(f\"{n}-Way Results for DR {dr}\")\n",
    "    cbar = ax.figure.colorbar(im, ax=ax)\n",
    "    cbar.ax.set_ylabel(\"\", rotation=-90, va=\"bottom\")\n",
    "    fig.tight_layout()\n",
    "    fig.set_size_inches(12, 12)\n",
    "    fig.savefig(f\"dr{dr}-{n}.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_heatmap(dr, p_range, offset):\n",
    "    r = build_standard_graph(dr, p_range, offset)\n",
    "    data, labels = binary(r)\n",
    "    show_heatmap(data, labels, dr)\n",
    "\n",
    "def n_way_heatmap(dr, p_range, offset, n):\n",
    "    r = build_standard_graph(dr, p_range, offset)\n",
    "    data, labels = n_way(r, n)\n",
    "    show_nway(data, labels, dr, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper Functions for Random Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sel(word_bank, n): #1 <= n <= 587\n",
    "    l = random.sample(word_bank, n)\n",
    "    bank = dict()\n",
    "    for word in l:\n",
    "        bank[word] = word_bank.index(word)\n",
    "    return bank\n",
    "#write random n-way to reorganize alphabetically and shift non-keys to end\n",
    "\n",
    "def random_nway(results, n):\n",
    "    word_key = np.load('./demo_stim/logits_to_word_key.npy')\n",
    "    #print(word_key)\n",
    "\n",
    "    results = results_to_nway(results, n)\n",
    "\n",
    "    #convert to numpy array\n",
    "    #x_words = set()\n",
    "    #y_words = set()\n",
    "    words = set()\n",
    "\n",
    "    for keys in results:\n",
    "        words.add(keys)\n",
    "        #x_labels.append((keys, ))\n",
    "        #for key in results[keys]:\n",
    "            #words.add(key[0])\n",
    "    y_adds = set()\n",
    "    for keys in results:\n",
    "        for key in results[keys]:\n",
    "            if key[0] not in words:\n",
    "                y_adds.add(key[0])\n",
    "    xlabels = sorted(list(words), key = lambda x: word_key[x].decode('UTF-8'))\n",
    "    y_adds = sorted(list(y_adds), key = lambda x: word_key[x].decode('UTF-8'))\n",
    "    ylabels = xlabels + y_adds\n",
    "\n",
    "    m = len(ylabels)\n",
    "    n = len(xlabels)\n",
    "\n",
    "    #print(ylabels)\n",
    "    #print(xlabels)\n",
    "\n",
    "    #rows results, columns expected\n",
    "    data = np.zeros((m, n))\n",
    "\n",
    "    for key in results:\n",
    "        sub_arr = results[key]\n",
    "        xpos = xlabels.index(key)\n",
    "        s = 0\n",
    "        for i in range(len(sub_arr)):\n",
    "            s += sub_arr[i][1]\n",
    "\n",
    "        for i in range(len(sub_arr)):\n",
    "            ypos = ylabels.index(sub_arr[i][0])\n",
    "            data[ypos][xpos] = sub_arr[i][1] / s\n",
    "\n",
    "    for i in range(len(xlabels)):\n",
    "        xlabels[i] = word_key[xlabels[i]].decode('UTF-8')\n",
    "    for i in range(len(ylabels)):\n",
    "        ylabels[i] = word_key[ylabels[i]].decode('UTF-8')\n",
    "\n",
    "    #print(labels)\n",
    "    return data, xlabels, ylabels\n",
    "\n",
    "def show_random(data, xlabels, ylabels, dr, n):\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(data)\n",
    "\n",
    "    #ax.set_xticks(np.arange(data.shape[1]))\n",
    "    #ax.set_xticklabels(xlabels, rotation=-90)\n",
    "    #ax.set_yticks(np.arange(data.shape[0]))\n",
    "    #ax.set_yticklabels(ylabels)\n",
    "    ax.set_title(f\"{n}-Way Results for DR {dr}\")\n",
    "    cbar = ax.figure.colorbar(im, ax=ax)\n",
    "    cbar.ax.set_ylabel(\"\", rotation=-90, va=\"bottom\")\n",
    "    fig.tight_layout()\n",
    "    fig.set_size_inches(12, 12)\n",
    "    fig.savefig(f\"dr{dr}-{n}.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_test(n, drs, p_range, graph = True):\n",
    "    #Build the unchanging parts of the model\n",
    "    net_object, word_key = load_model()\n",
    "    \n",
    "    word_bank = random_sel(word_key, n)\n",
    "    ds = []\n",
    "\n",
    "    for dr in drs:\n",
    "        bp = os.path.join(\"graph_data\", f\"dr{dr}\")\n",
    "        t_dir = os.path.join(bp, \"txts\")\n",
    "        c_dir = os.path.join(bp, \"cgrams\")\n",
    "        \n",
    "        results = dict()\n",
    "        txt_set = []\n",
    "\n",
    "        for f in os.listdir(t_dir):\n",
    "            sp_des, title = extract_text(f)\n",
    "            info = sp_des + title\n",
    "            t_path = os.path.join(t_dir, f\"{info}.txt\")\n",
    "            s = check_txt(t_path, word_bank, 3, 7)\n",
    "            if s:\n",
    "                txt_set.append((info, s))\n",
    "        \n",
    "        for t in txt_set:\n",
    "            info = t[0]\n",
    "            word_set = t[1]\n",
    "            case = -1\n",
    "            success = False\n",
    "            for sd in os.listdir(c_dir):\n",
    "                sd_path = os.path.join(c_dir, sd)\n",
    "                if os.path.isdir(sd_path):\n",
    "                    c_path = os.path.join(sd_path, f\"{info}-{sd}.npy\")\n",
    "                    if os.path.isfile(c_path):\n",
    "                        case += 1\n",
    "                        cgram = np.load(c_path)\n",
    "                        logits = run_cgram(cgram, net_object)\n",
    "                        inds = distrib(logits, p_range)\n",
    "\n",
    "                        res, dist = result_dist(word_set, word_key, inds)\n",
    "                        if res:\n",
    "                            success = True\n",
    "                            target = dist[len(dist) - 1]\n",
    "                            results = merge_dist(results, target, dist)\n",
    "            if not success:\n",
    "                cases = [[3,4], [3,4,5], [3,4,5,6,7]]\n",
    "                for key in word_set:\n",
    "                    for pos in cases[case]:\n",
    "                        if pos in word_set[key]:\n",
    "                            target = word_key.index(key)\n",
    "                            results = merge_dist(results, target, dist)\n",
    "                            break\n",
    "        ds.append(results)\n",
    "        if graph: #this being set to true messes up return\n",
    "            data, xlabels, ylabels = random_nway(copy.deepcopy(results), 5)\n",
    "            show_random(data, xlabels, ylabels, dr, 5)\n",
    "    return ds, word_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fucntions to look at results data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Helpers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_self(word, dic):\n",
    "    if word in dic:\n",
    "        return dic[word]\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_words(r, wb): #retrieves all words who never showed up in their own inputs\n",
    "    print(f\"{len(r)} total words.\\n\")\n",
    "    zeros = []\n",
    "    for key in r:\n",
    "        if check_self(key, r[key]) == 0:\n",
    "            zeros.append(wb[key])\n",
    "    return zeros\n",
    "\n",
    "def display_zero(zeros, dr):\n",
    "    wstr = ''\n",
    "    for word in zeros[:len(zeros) - 1]:\n",
    "        wstr += f'{word}, '\n",
    "    wstr += f\"{zeros[len(zeros) - 1]}.\"\n",
    "    print(f\"{len(zeros)} words with 0 score in dialect {dr}, words are '{wstr}'\\n\")\n",
    "\n",
    "def worst_words(r, wb, n): #retrieves n-worst non-zero results\n",
    "    worst = []\n",
    "    for key in r:\n",
    "        val = check_self(key, r[key])\n",
    "        if val != 0:\n",
    "            if len(worst) < n:\n",
    "                worst.append((wb[key], val))\n",
    "                worst = sorted(worst, key = lambda x: x[1])\n",
    "            else:\n",
    "                if val < worst[n-1][1]:\n",
    "                    worst.pop()\n",
    "                    worst.append((wb[key], val))\n",
    "                    worst = sorted(worst, key = lambda x: x[1])\n",
    "    return worst\n",
    "\n",
    "def display_worst(worst, dr, n):\n",
    "    print(f\"{n} worst-predicted words displayed in order from worst to best.\")\n",
    "    for i in range(n):\n",
    "        w = worst[i][0]\n",
    "        score = worst[i][1]\n",
    "        print(f\"Word number {i}: {w}. Score: {score}.\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shared_zeros(arr):\n",
    "    zeros = dict()\n",
    "    for l in arr:\n",
    "        for z in l:\n",
    "            if z in zeros:\n",
    "                zeros[z] += 1\n",
    "            else:\n",
    "                zeros[z] = 1\n",
    "    return zeros\n",
    "\n",
    "def format_sz(sz):\n",
    "    zeros = []\n",
    "    for zero in sz:\n",
    "        zeros.append((zero, sz[zero]))\n",
    "    zeros = sorted(zeros, key = lambda x: x[1])[::-1]\n",
    "    print(f\"Shared zeroes from greatest to least frequency:\")\n",
    "    for pair in zeros:\n",
    "        print(f\"{pair[0]}: {pair[1]}\")\n",
    "    print('\\n')\n",
    "\n",
    "def shared_worsts(arr):\n",
    "    worsts = dict()\n",
    "    for l in arr:\n",
    "        for w in l:\n",
    "            if w[0] in worsts:\n",
    "                worsts[w[0]] += 1\n",
    "            else:\n",
    "                worsts[w[0]] = 1\n",
    "    return worsts\n",
    "\n",
    "def format_sw(sw):\n",
    "    w = []\n",
    "    for worst in sw:\n",
    "        w.append((worst, sw[worst]))\n",
    "    w = sorted(w, key = lambda x: x[1])[::-1]\n",
    "    print(f\"Shared worsts from greatest to least frequency:\")\n",
    "    for pair in w:\n",
    "        print(f\"{pair[0]}: {pair[1]}\")\n",
    "    print('\\n')\n",
    "            \n",
    "def comb_shared(r):\n",
    "    sums = dict()\n",
    "    iters = dict()\n",
    "    for dr in r:\n",
    "        for key in dr:\n",
    "            val = check_self(key, dr[key])\n",
    "            if key in iters:\n",
    "                iters[key] += 1\n",
    "                sums[key] += val\n",
    "            else:\n",
    "                iters[key] = 1\n",
    "                sums[key] = val\n",
    "    combo = []\n",
    "    for key in sums:\n",
    "        combo.append((key, sums[key] / iters[key]))\n",
    "    combo = sorted(combo, key = lambda x: x[1])\n",
    "    return combo\n",
    "\n",
    "def disp_shared(combo, n, wb):\n",
    "    print(f\"Lowest {n} results overall.\")\n",
    "    for i in range(min(n, len(combo))):\n",
    "        w = combo[i][0]\n",
    "        score = combo[i][1]\n",
    "        print(f\"Word number {i+1}: {wb[w]}. Score: {score}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gathers data on specific words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drs = [i for i in range(1,9)]\n",
    "r, wb = random_test(587, drs, 5, False)\n",
    "n = 10\n",
    "zeros = []\n",
    "worsts = []\n",
    "#print(wb)\n",
    "for i in range(1,9):\n",
    "    res = r[i - 1]\n",
    "    zero = zero_words(res, wb)\n",
    "    zeros.append(zero)\n",
    "    display_zero(zero, i)\n",
    "    worst = worst_words(res, wb, n)\n",
    "    worsts.append(worst)\n",
    "    display_worst(worst, i, n)\n",
    "sz = shared_zeros(zeros)\n",
    "format_sz(sz)\n",
    "sw = shared_worsts(worsts)\n",
    "format_sw(sw)\n",
    "disp_shared(comb_shared(r), 50, wb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swaps(r):\n",
    "    swaps = dict()\n",
    "    c = 0\n",
    "    for dr in r:\n",
    "        c += 1\n",
    "        for word in dr:\n",
    "            #print(word)\n",
    "            wDict = dr[word]\n",
    "            #print(wDict)\n",
    "            for pred in wDict:\n",
    "                if pred in dr and word != pred and word in dr[pred]:\n",
    "                    v1 = wDict[pred]\n",
    "                    v2 = dr[pred][word]\n",
    "                    pair = (word, pred)\n",
    "                    res = (v1, v2, c)\n",
    "                    if pair in swaps:\n",
    "                        swaps[pair].append(res)\n",
    "                    else:\n",
    "                        swaps[pair] = [res]\n",
    "    return swaps\n",
    "\n",
    "def proc_swaps(swaps, wb):\n",
    "    for pair in swaps:\n",
    "        p1, p2 = pair[0], pair[1]\n",
    "        l = len(swaps[pair])\n",
    "        print(f\"{l} swap(s) for pair '{wb[p1]}' and '{wb[p2]}'.\")\n",
    "        for i in range(l):\n",
    "            print(f\"For swap number {i + 1} the strengths were {swaps[pair][i][0]} and {swaps[pair][i][1]}, in dr {swaps[pair][i][2]}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_success_graph(r): #graphs for each word what its prediction is in all 8 dialects\n",
    "    data = np.zeros((8, 587))\n",
    "    for i in range(8):\n",
    "        for word in range(587):\n",
    "            dr = r[i]\n",
    "            if word in dr:\n",
    "                subDict = dr[word]\n",
    "                data[i][word] = check_self(word, subDict)\n",
    "            else:\n",
    "                data[i][word] = float(\"NaN\")\n",
    "    #print(data)\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(data)\n",
    "\n",
    "    ax.set_title(f\"Total Results\")\n",
    "    cbar = ax.figure.colorbar(im, ax=ax)\n",
    "    cbar.ax.set_ylabel(\"\", rotation=-90, va=\"bottom\")\n",
    "    fig.tight_layout()\n",
    "    fig.set_size_inches(12, 12)\n",
    "    fig.savefig(f\"full success.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def failure_graph(r, f_thresh, n_thresh, wb):\n",
    "    fs = dict()\n",
    "    for i in range(8):\n",
    "        dr = r[i]\n",
    "        for key in dr:\n",
    "            if check_self(key, dr[key]) <= f_thresh:\n",
    "                if key in fs:\n",
    "                    fs[key] += 1\n",
    "                else:\n",
    "                    fs[key] = 1\n",
    "    words = []\n",
    "    for key in fs:\n",
    "        if fs[key] >= n_thresh:\n",
    "            words.append((key, fs[key]))\n",
    "\n",
    "    words = sorted(words, key = lambda x: x[1])[::-1]\n",
    "    l = len(words)\n",
    "    x = [i for i in range(l)]\n",
    "    heights = []\n",
    "    x_labels = []\n",
    "    for i in range(l):\n",
    "        heights.append(words[i][1])\n",
    "        x_labels.append(wb[words[i][0]])\n",
    "    \n",
    "    #print(x_labels)\n",
    "    #print(heights)\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.bar(x, heights)\n",
    "    ax.set_ylabel(\"No. of Failures\")\n",
    "    ax.set_xticks(np.arange(len(x_labels)))\n",
    "    ax.set_xticklabels(x_labels, rotation=-90)\n",
    "    ax.set_title(f\"Failure Graph\")\n",
    "    fig.tight_layout()\n",
    "    fig.set_size_inches(12, 12)\n",
    "    fig.savefig(f\"failure.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def av_dialect_score(r):\n",
    "    s = []\n",
    "    for i in range(8):\n",
    "        scores = 0\n",
    "        total = 0\n",
    "        dr = r[i]\n",
    "        for key in dr:\n",
    "            #print(key, dr[key])\n",
    "            scores += check_self(key, dr[key])\n",
    "            total += 1\n",
    "        print(f\"Score for dr {i+1}: {scores/total}.\")\n",
    "        s.append(scores/total)\n",
    "    return s\n",
    "\n",
    "def graph_di_score(s):\n",
    "    dialects = ['New England', 'Northern', 'North Midland', 'South Midland', 'Southern', 'NYC', 'Western', 'Army Brat']\n",
    "    l = len(s)\n",
    "    m = max(s)\n",
    "    x = [i for i in range(l)]\n",
    "    heights = []\n",
    "    x_labels = []\n",
    "    for i in range(l):\n",
    "        heights.append(s[i] / m)\n",
    "        x_labels.append(dialects[i])\n",
    "    \n",
    "    #print(x_labels)\n",
    "    #print(heights)\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.bar(x, heights)\n",
    "    ax.set_ylabel(\"Relative Success\")\n",
    "    ax.set_xticks(np.arange(len(x_labels)))\n",
    "    ax.set_xticklabels(x_labels, rotation=-90)\n",
    "    ax.set_title(f\"Dialectical Results\")\n",
    "    fig.tight_layout()\n",
    "    fig.set_size_inches(12, 12)\n",
    "    fig.savefig(f\"dialects.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "drs = [i for i in range(1,9)]\n",
    "r, wb = random_test(587, drs, 5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_di_score(av_dialect_score(r))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.13 ('kell36')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "98e25d2b9ef09a96a2b0543d994b062752799b128e644637d3c12b0aacd3cf1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
